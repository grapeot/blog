Title: 从上下文失忆到文档驱动开发：突破Agentic AI的项目规模陷阱
Date: 2025-03-09 17:00
Category: Computing
Tags: AI, AI Native, Agentic AI, Chinese
Slug: agentic-memory

自2025年伊始，Cursor、WindSurf、Trae等Agentic AI编程工具开始席卷开发领域。然而与过往的GenAI技术类似，这些Agentic AI技术同样面临着小规模demo惊艳，产品化实战翻车的困境——它们生成一两千行的小型原型轻而易举。自我迭代、自动Debug、快速交付，整个过程行云流水。可一旦踏入真实的软件工程应用，比如当项目规模突破5000行代码时，Agentic AI的魔法似乎突然失灵。它就像在迷雾中盲行，既看不清全局架构，又记不住历史逻辑，最终代码中充斥着诡异的故障模式，开发者不得不频繁手动干预才能勉强达标。

这不禁让人怀疑：Agentic AI编程究竟是一场改变游戏规则的革命开端，还是又一个华而不实的技术泡沫？这篇文章将深入探讨两个核心问题：

1. 为什么Agentic AI会有项目规模的限制？
2. 有哪些技术路径可能突破这一规模限制，让AI在大型工程场景中真正发挥实用价值？

## 三大Failure Pattern：空间错配、时间遗忘、重复造轮子

想要回答为什么的问题，我们首先得具体看看Agentic AI在大规模工程中究竟有哪些Failure Pattern。一般来说，我们会遇到三种模式。

第一种模式是软件开发的空间维度，改一个文件，却忘了另一个文件的逻辑。在只有几百行代码的小型demo里，Cursor一次就能输出一个可用的版本，非常灵活可靠。但是当项目到了几千上万行的时候，它常常会犯一些初级错误。比如，明明另一个文件里已经有了现成的功能实现，它却视而不见，又从头开始写了另一个函数。或者它在一次迭代中对A模块做了修改，却没有注意到B模块正依赖着那个接口，最终导致A、B无法兼容。即使通过Agentic Workflow它可以看到相关的出错信息，但仍然感觉整个AI跟被降智了一样，也需要长时间的反复迭代才能发现问题，并且最终修复。

这一定程度上是因为上下文窗口所限制的。Cursor等工具是根据一定规则自动化构建上下文窗口的。当我们让AI写代码的时候，如果因为种种原因，另一个已经有了相关代码的文件没有被放进上下文窗口，它自然没办法在实现中考虑到这一因素。因此就会出现上面所说的这种问题。

第二种模式是时间维度的反复，纠错又推翻，推翻再纠错。如果你用Agentic AI开发过相对复杂的横跨几天的项目的话，你也许已经注意到了它的一种自我推翻的模式。比如，它第一次写程序犯了一个错误A，在程序跑挂以后，会说“好的，我来通过修改X来修正。”结果过一阵子在进行其他改动的过程中，它忽然又把X改回去了。所以这时候错误A又再次出现，它就只能重新再debug。同时，随着工程的增大，整个AI给人的感觉也越来越笨，在工程中后期debug的效率远远不如前期。所以这样反反复复不仅消耗时间和耐心，而且还可能让工程进展陷入困境。

这背后的原因其实是上下文窗口的限制。现在的Agentic AI的工具往往还是把上下文窗口作为一个记忆的媒介。开始的时候，错误A和修正X仍然在上下文窗口里，因此这时候Agentic AI仍然记得不要去推翻X这个实现。但是因为种种原因，比如对话太长，或者新开了一个会话，这个记忆被抛出窗口以后，AI就忘了这个历史的context，忘了自己为什么要使用X。于是就再次回到老路，推翻了之前的修复。

第三种模式是，这两个问题在对接已有代码库的时候尤其严重，尤其是当这些代码不是AI自己写的时候。它往往缺乏一种全局的视角，没办法理解这个代码库的高层设计。这在空间上表现为无法准确定位某个需求所需要更改的具体文件；在时间上，它往往也没有这个代码库发展历史上的context。因此，在AI写代码的过程中，它往往会体现出一个坏毛病：宁愿再写一个功能，也不去复用或者理解原先的代码。哪怕这个原先的代码有时候是它自己写的，它也陷入这种鬼打墙的境地。最终结果就是同一功能被重复实现多次，逻辑还可能互相冲突。

综合来看，当代码量突破一定程度，比如5000行的时候，Agentic AI的长期记忆力就开始成为整个生产力的瓶颈。工程越到后期，改动越多，上下文越庞杂，AI就越像在迷雾中摸索，一边推翻自己，一边又忘记自己探索过什么。接下来让我们看看，这种上下文失忆究竟源于什么技术限制。

## 核心限制：依存于上下文窗口的短期记忆

要理解Agentic AI在大规模项目上的失效，先得看看它的记忆机制：大多数Agentic AI目前都依赖有限的上下文窗口来获取先前写过的代码或决策信息。不论上下文窗口是RAG（Retrieval-Augmented Generation）构建的，还是基于Agentic方法自动读取文件，只要关键内容没有被完整纳入其中，AI就会遗忘那部分信息，自我推翻和反复犯错的模式就会再度出现。

当项目越来越大，一种直观的想法就是不断地重构：把代码切分得更细，好让AI在一次输出时更容易专注。然而这种方法只能缓解局部问题，并不能在根本上解决全局设计理解不足的挑战。就算代码再整洁，AI依然依赖短期上下文来思考，一旦上下文窗口溢出，便忘却先前的逻辑。换言之，重构只是减少了AI在某个时刻需要处理的信息量，却并没有让AI真正拥有一种可以反复调用的长久记忆。

所以，当前GenAI基于上下文窗口的工作的机制就决定了，它好比是一个只有七秒记忆的鱼，每次只能记住当前一小段信息，游过这道门，就忘了先前经历，导致它不断犯下同样的错误。它可以记住短时间内写过的代码。但如果没有长期记忆，Agentic AI对于规模超过几千行的项目，注定就只能处于半瘫痪状态。问题指向的核心是：我们到底应该如何改变AI的记忆的实现机制？不仅仅依靠短时间的上下文窗口来给它提供瞬时记忆，而同时用另外一种方式来提供一种长期记忆的机制。

和其他问题类似，这个解决方法也可以从我们人类自身寻找。我们人类也有类似的问题：我们的短期记忆容量也很有限，但是我们通过“好记性不如烂笔头”这种方法，解决了这个问题。在代码之外，我们有额外的文档来记录全局的设计和历史的决策。这些文档，包括Tribal Knowledge，构成了我们的经验和长期记忆。这样让我们人类不会有特别严重的自我推翻的模式，也让我们有了更强的全局视角。而这种长期记忆，也是解决Agentic AI当下困境的核心。

## 破局之道：为Agentic AI构建长期记忆

关于如何具体构建AI的长期记忆机制，这是一个非常开放的问题。我也没有一个成熟的解决方案，但是也许可以从下面三个方面来思考：

第一是Document-Driven Development，文档驱动开发。这个思路是对前面烂笔头带来好记性的模仿。我们可以通过Prompt Engineering等方法来让AI知道，交付一个文档是和交付代码同等重要的事情。AI不仅需要写代码，另一个重要任务是时刻维护一个文档。这个文档主要作用是定义这个项目的外部行为、产品决策、技术框架、高层设计，同时还要提供它历史上的一些context，比如之前做过哪些尝试，效果如何等等。这样，当 AI 在写代码的时候，就可以更高效地利用上下文窗口。它只要看文档，就可以迅速地知晓整体架构，理解相关依赖，而不必一遍遍地把所有原文件塞进上下文窗口。而类似的，我们也可以让AI在写代码的时候，先更新文档然后再根据文档去改代码，时刻保证文档的内容和代码的内容是吻合的。注意这个长期记忆未必要是纯自然语言的形式，它也可以是软件工程中常见的 UML 图，或者协议的可视化，甚至是一个 JSON 文档等，让人类和电脑都可以理解的格式。

第二是，这个文档不仅对于单 Agent 有用，对Multi-Agent 也是一个重要的技巧。在 Multi-Agent 的技术架构中，我们[之前讨论过](https://yage.ai/multi-agent.html)，它的核心好处在让不同的 Agent 有彼此隔离的上下文窗口。而这就需要不同的 Agent 之间有一个高效精准的通信渠道。这里所提到的长期记忆就是一个理想的渠道。它可以一方面让每个 Agent 从高层抽象的角度知道其他 Agent 干了哪些事，从而不会被其他 Agent 工作的细节所干扰。另一方面，也形成了一个横跨所有 Agent 的 single source of truth。

因此，我们在实现这个长期记忆的时候，也要额外注意维护多个 Agent 之间的 consistency，比如引入一些锁机制，让多个 Agent 同时读写的情况下也能有数据的一致性。这有点类似于多人协作软件的概念，只是把人工换成了 AI。我们如何在这个过程中避免冲突、自动合并历史和进行差异分析，也是一个有待探索的路径。

第三个角度则是人类在这个过程中应当扮演什么角色。短期内也许我们不应该期待 Agentic AI 可以成为一个完全自主工作的系统。就好比一个有经验的人类员工也需要领导不断的指导和 coach 一样。我们对待 AI 的态度不应该是用完即抛，给它个任务就放手不管。而应该是给它适当的引导、纠正，甚至培养，让它在工作中不断积累经验。而长期记忆的出现则给我们提供了新的可能。我们和 AI 的沟通不仅仅局限于通过 context window（也就是聊天）来进行。我们甚至可以把这种长期记忆作为一种沟通的媒介。例如，当我们发现 AI 的行为跟我们的预期有比较大的差别的时候，我们不是通过对话来一点一点纠正它，而是直接更改它的整个设计文档，然后指示它根据这个文档重写，纠正所有代码与文档不一致的地方。或者如果我们发现它有一个常犯的错误，我们也可以直接修改它的长期记忆，把类似的教训写进去。而这种长期记忆甚至是可以共享和分发的，如果一个 AI 学到了一个经验教训，别的在同一个项目下工作的 AI 也能自动得到相应的更新，这本身也是一个非常有意思和重要的改变。

换言之，这个长期记忆不是简单地说我们让AI写写文档，看看文档就结束了，而是同时还要考虑到它怎样融入人与AI协同工作的工作流里。目前Cursor等工具走的还是一种AI主导的道路。人给一个简单的需求，他们就去把整个工程实现出来。但对于更复杂的项目，有可能我们需要走一条人机配合的道路。比如，人类来主导文档体系的结构和关键摘要，AI来协作补充细节。当系统需要重大改动的时候，AI先尝试更新文档，给一个文档的草稿，人类做修订和最终确认。AI再根据这个文档进行更多的更改。这样也许是一条更科学的道路，我们又不至于完全依赖AI的上下文窗口这样短时记忆，也不用过分纠结过多的琐碎细节。

## 结语

我们现在还处于整个Agentic AI的萌芽时期。它已经给了整个软件开发领域很大的鼓舞和惊喜，但同时也带来了更多的问题。这篇文章并没有提供一个成熟的解决方案，而可能只是单纯抛出了更多的问题。

长期记忆的设计和具体形态的确定也不是一蹴而就的事情。要具体定位Agentic AI发展的瓶颈和未来的技术路线，我们的当务之急也许不是去拍脑袋想可能的方案是什么，然后就着手实现。而是先在目前的工具中加入更多的透明度、可解释性和可调试性。例如，在Cursor或者Trae等工具中把context window给暴露出来，让各种翻车模式更好理解。另外一方面，也可以引入社区的力量，让爱好者或者深度用户有能力去影响和控制这些context window，甚至加入他们自己对于长期记忆的构想。当然，对于各个Agentic AI厂商而言，这确实也有相当大的商业风险。但是，群策群力，开放透明的模式不失为一条在短期内快速提升整个领域对于Agentic AI认知和实践的可行道路。

总而言之，Agentic AI确实在5000行以上会面临翻车隐患，本质是记忆完全依赖于上下文窗口。而要真正突破，文档驱动开发、开放透明和社区共建可能是有前景的方向。

<script async data-uid="65448d4615" src="https://yage.kit.com/65448d4615/index.js"></script>