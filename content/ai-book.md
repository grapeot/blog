Title: 我们让AI读完三年微信群聊记录，它写了一本出人意料的小书
Date: 2025-04-15 23:00
Category: Computing
Tags: AI, Chinese
Slug: ai-book

过去这两年，我们不停用大模型写总结、写PRD、写代码、写诗……但始终有一种感觉：这离AI成为生产力还差点什么。
这不是因为模型不够强。GPT已经能写出相当漂亮的文章、生成复杂的函数、甚至分析财务报表。可每次让它干点真正有用的事，比如复盘一个产品发布的问题、帮忙理解过去一年的决策脉络、为团队输出一份靠谱的协作指南——它就开始打滑。

我们其实一直在追问一个问题：为什么AI还不能真正参与到复杂认知中？
起初我们以为是Prompt写得不够好，是模型还不够聪明，是我们没把任务拆清楚。但渐渐我们意识到，问题可能更深一层：AI所接触到的世界，是不完整的。

## AI为什么还不能成为生产力？

这事说简单点，其实是我们在用人类方式组织信息，却指望AI能像人一样理解它。
人类有隐性记忆，有情境直觉，有tribal knowledge。

*   你见过老板皱眉头，就知道这个方向他不喜欢；
*   你和同事喝一杯咖啡，就能交换一堆未来计划里的模糊想法；
*   你看过10个PR，心里大概知道谁对系统架构有判断，谁在划水。

这些东西，我们不写在文档里，也不需要写。
但AI不行。它没有长期记忆。没有上下文感知。你不给它文件，它连这个团队做过什么都不知道。
所以在很多我们看来非常简单的任务上，AI反而完全失能。不是它不聪明，是我们从没把世界讲清楚。
这就是我们迟迟无法让AI真正接手复杂任务的原因。

## 信息废墟：AI最难啃的骨头

有一种语料，是AI最怕的，也是我们最多的：

*   三年Slack频道的聊天记录
*   一个微信群里各种争论、闲聊、转发、贴图和冷笑话
*   产品需求的群内反复改动痕迹
*   工程师对线式的PR comment线程
*   甚至是用户社区的讨论板

这些数据有一个共同点：
它们很沉、很碎、很杂，却又不可替代。
我们称之为**信息废墟型数据**。

这些语料本质上具备三个特征：

1.  全时浸润（Immersive）：是团队真实决策轨迹的主通道；
2.  极度非结构化：没有主题、没有thread、没有明确对象；
3.  低信噪比：无效信息极多，人工都读得头疼。

但也正因为如此，它们极度宝贵。它们藏着团队的知识地图、决策权重、文化惯性——这些才是真正不可替代的组织智慧。
问题是，AI对这种语料，几乎束手无策。

![Information Ruin](/images/ai_book_ruin.png)

## 我们尝试了一切……都不太行

我们不是一开始就想让AI写一本书。
我们最早的目标非常朴素：能不能让AI帮我们把微信群里的讨论捋顺一点？我们已经手动做了一些整理，但太累、太慢、太费。
第一步我们想到的是RAG。这个几乎是当时所有人尝试结构化内容的起点：把语料切成小段，建立embedding，然后配合Prompt进行生成式回答。
结果是，模型确实能回答谁说了什么，有没有人讨论过X这样的问题。但当我们问它：
"这三年大家对AI协作的理解是怎么演化的？有没有什么争议点和统一点？"
它完全没法回答。不是能力不行，是它根本没有一个完整的全景图可以从中提出判断。

我们后来试了Agentic Workflow。
理论上，Agent可以自己规划任务、自主调用工具、多轮思考，听起来很棒。但问题是：
在这种高度非结构化的数据里，Agent根本不知道下一步干什么。
因为语料太乱，线索太碎，没有明显目标，也没有明确可引用的信息块。结果就是：

*   要么什么都找不到
*   要么永远在提取一小段再总结
*   要么只能反复空转："我找到了一个跟你说的东西类似的内容……"

这让我们开始怀疑：是不是我们对AI的期待出了问题？是不是我们喂给它的世界，根本不适合它理解？

## 拐点出现：GPT-4.1 和 Gemini 2.5 的发布

我们真正的突破，其实来的非常无声无息。
某种意义上讲，GPT-4.1的发布并没有引起太多轰动。它不像之前版本那样有炫技式能力升级，也没有引入新的交互方式或模型形态。
但它带来了两个关键变化：

1.  支持百万Token的上下文窗口
2.  同时支持Prompt caching，让成本下降了一个数量级，

同样拥有百万Token的Gemini 2.5也在几周前发布，表现得比之前版本更聪明、更主动。虽然它还不支持prompt caching，费用很吓人，但能力确实强悍。

这时候我们突然意识到：
这是历史上第一次，有模型同时具备两个关键属性：记忆容量 + 智能质量。
这两点如果分开出现，其实并不能改变格局：

*   o1很聪明，但只记200k Token，无法hold住低密度语境
*   Gemini 2.0记得多，但常常输出混乱，缺乏判断力

我们之前的尝试之所以失败，归根结底就是没有一个模型能理解一个混乱的世界。
现在不一样了。
我们有了一种可以读完全部聊天记录，再深思熟虑地说："我觉得你们讨论的这个问题，值得单独写一章"的模型。
我们决定试试看。
我们把三年微信群聊的内容打包，甚至没有清洗，而是作为整体扔进模型里。我们没有分chunk，也没有预设问题，只是直接放进上下文窗口。

故事，从这里开始变得不同了。

## 反思-提取-构建：一次有点反人类直觉的协作方式

我们并没有一下子就生成一本书。
其实最开始，我们只是问模型："你从这堆聊天记录里学到了什么？"我们也没太指望它能说出什么新鲜东西。毕竟，从工程角度看，它面对的只是40多万token的杂乱文本。
但出乎意料，它说了一些还挺像回事的话。
它提到了几个我们之前也有过争论、但未曾总结出来的主题：

*   AI在团队协作中的人格差异如何影响使用方式
*   Prompt作为管理方式的隐喻（你不是写指令，而是在设定绩效考核）
*   多模型协作中的心智模型构建问题

我们突然意识到：模型不是在回答问题，它更像是在整理阅读体会。
所以我们把这个流程固化成一个循环：

1.  Seed Idea输入：我们给它一点方向，比如"你觉得团队在使用AI上，有什么模式值得注意？"
2.  观点生成：模型输出若干观点
3.  反问/检验：我们要求它为这些观点找出群聊中具体证据，或者找反例
4.  再次精炼：模型更新观点，筛选出几个更成熟、更具层次的角度

这个循环我们跑了很多轮，每轮不超过10000 token，每一轮模型都读了一部分之前的历史。
等这些观点沉淀得差不多了，我们再让它根据每个观点，分别写一章内容。每章的长度大约在4000~5000字，总共写了大约七章。
最后拼合、润色，形成了这本小书。

我们在过程中还做了一些Prompt Engineering的微调：

*   把每章写作任务拆成1-3章、4-6章两轮执行，规避token过多导致偷懒
*   每次写作前都让模型复述目标，增强任务一致性
*   对结尾偷懒问题采用反向诱导："请在结尾写上'未完待续'"，然后我们后处理时把它删掉

这些技巧看起来像是小聪明，但却是能让模型最终写出高质量文章的关键。

## 写出来的东西，出乎意料地像人
我们原本以为，[这本小书](https://www.superlinear.academy/c/ai-resources/ai-book-draft)大概率只是一个拼贴体。
但当我们真正通读下来时，发现了几个很令人惊讶的点：

1.  它的节奏感是自然的：章节之间并不重复，逻辑衔接流畅。
2.  它有观点，而不是流水账：有些段落甚至会自我修正，比如"在前面我们说X，但回头看，其实可能也有反例Y"。
3.  它有结构感：从 我们在用AI做什么 → AI正在如何重塑组织结构 → 未来可能的协作范式。这是一个明显走得越来越远的思想展开。

我们当然不能说这是一本出版意义上的好书。但我们确实可以说，这是一个语言模型第一次在非结构语料中，构建出了一个认知结构。
这不只是技术成就感的问题，而是我们第一次觉得，AI不再是接受指令的人，而是一个被允许思考的阅读者。

## 智能 ≠ 算力，智能 = 记忆 × 沉浸

这个实验让我们重新思考了一个问题：AI的智能，究竟来自哪里？
我们曾经以为：更大模型、更强推理、更复杂agent调度，是让AI更有用的路径。
但我们现在认为：真正的突破点，不在模型，而在信息流。

过去，AI每次只能看到一小段内容。我们强行用RAG拼接、用Memory机制缓存，其实只是做了一种碎片式模拟阅读。
而1M Token context window带来的，是一种沉浸式、无中断的深度处理能力。
就像人阅读一样。
你不会每次翻书都只读三页，还不停让人把接下来的页找出来再拼好。你会一口气看完，反思，再看一遍。
现在AI也终于可以这样看了。
记忆力和沉浸时间，是智能爆发的土壤。不是更强的推理能力带来了理解，而是理解终于有了土壤。
我们不需要更多神经元，我们需要更完整的输入。

![Memory Flower](/images/ai_book_memory_flower.png)

## 所以，这到底意味着什么？

我们当然可以说，我们做了一本书。但这其实不是重点。
重点是，我们看到了一个可能性：AI不再需要我们去为它清理世界，而可以直接面对一个原生态的现实，并开始理解它。
这意味着什么？

*   意味着我们可以不再手动清洗客户采访纪要，直接让AI找出产品机会；
*   意味着我们可以不再把团队决策写进PPT，而是直接让AI从Slack中整理出复盘报告；
*   意味着我们甚至可以用AI去读一个项目的所有Pull Request，写出一份代码演进史；

这不是未来。这是现在。
我们做的那本书，并不是为了让大家读。它是我们和模型一起做的第一份知识产品协作。是一次试图把人类语言的残渣，变成机器可以理解的逻辑结构 的努力。
我们失败过很多次。试过错的方法。浪费了不少token。
但这次，我们好像真的打开了一道门。

## 尾声：写书只是表象，重构理解才是实质

AI未来到底会做什么？没人能准确预测。
但有一点，我们越来越确定：
AI的关键能力，不是写代码，不是生成图片，而是构建理解。
这不是生成式AI的下一步，它不是未来——它其实早就开始了。
我们只是试图把它说清楚一点。

你们团队有没有一个文档一直想整理却从没人有空？ 有没有一个内部讨论区，总觉得里面藏着战略思维的碎片？ 有没有一堆访谈录音，一直没时间听？
你可以试试丢给AI。别让它总结，别问它问题。
你就跟它说：你读完之后，告诉我——你从这些里面，学到了什么？
我们就是这么开始的。
[这本小书](https://www.superlinear.academy/c/ai-resources/ai-book-draft)，只是它给我们的第一个回答。

(本文由GPT-4.1执笔，鸭哥只提供观点，逻辑，结构，顺序和反馈)

<script async data-uid="65448d4615" src="https://yage.kit.com/65448d4615/index.js"></script>