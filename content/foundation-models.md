Title: 从历史的角度看基础大模型基础在何处
Date: 2024-08-06 21:30
Category: Computing
Tags: Chinese, AI
Slug: foundation-models

在ChatGPT发布以后，基础大模型（Foundation Model）这个词一下子火了起来。不论是各个自媒体，还是科研单位和企业，都在热火朝天地探讨基础大模型。腾讯研究院曾经做过一个不完全统计，现在中国有200多个基础大模型正在研发。但是，到底什么是基础大模型呢？一个普通的大规模模型要想成为基础大模型，需要满足什么样的条件呢？如果它的核心标准是"大"的话，需要多大才能称为基础大模型呢？这些问题讨论的人却不多，因此我写了这篇文章，想从回顾历史的角度来解释一下我的观点。

在2017年以前，深度学习已经成为机器学习的主流。相比于传统的机器学习算法，深度学习有一个核心的优势：它一直没有出现数据饱和的现象。传统的模型，即使不断增加模型的复杂程度，到了一定规模的数据之后，它也无法再有效地利用这些数据。具体表现就是，随着训练数据的增多，它的精度不再提高。这是整个机器学习领域面临的一个老大难问题。

但深度学习则完全没有这个问题。只要你给它更多的GPU，更多的训练数据，让它训练更复杂的模型，它就能保证一定会给你更好的精度。直到如今，我们也没有发现深度学习有任何饱和的迹象。这是为什么深度学习如此受各个公司欢迎，尤其是大公司的欢迎的原因。毕竟像这种只要砸钱就能拿到更好精度的事情，可以为他们构筑一个天生的护城河。

但深度学习的这一切都有一个前提。就是神经网络或者说Feedforward的Neural Network是一个特别适合并行的算法，所以它可以在GPU上高效地进行推理和优化。但直到2017年，在NLP领域流行的方法仍然是LSTM等Recurrent Neural Network。它的特点是要想计算下一个词的结果，必须先把前一个词的结果全部计算完，这就造成了一个问题：它是一个串行处理的算法，对GPU非常不友好，很难高效地并行优化。这是为什么虽然Auto Regressive Task很早就出现了——比如把一段话中间的一个词遮住，从上下文来猜这个词是什么——但一直没有在大规模的数据上推行开的原因。比如Word2Vec这种遮住一个词猜这个词是什么的模型只是一个两层的神经网络，并没有真正地和深度学习结合起来。

2017年，Google发布了Transformer这个架构，终于改变了这一切。Transformer通过全局的注意力机制，将LSTM串行的计算结构变成了一种可以进行并行处理的结构。因此它可以在GPU上进行高效的训练。这为NLP领域带来了新的思路。因此，仅仅在几个月之后，OpenAI和Google分别独立发布了GPT和BERT两种算法，通过引入大规模预训练，极大提高了所有NLP任务的算法表现。这些算法的使用一般分为两个步骤：

1. 第一步是用之前提到的在一段话中间遮住一个词来预测这个词是什么，在大规模的数据上面做无监督训练，从而得到一个预训练的神经网络。
2. 在此基础上，第二步是针对具体的任务，比如对一段话进行情感分析，进行进一步的模型微调。让这个基准模型能够适配到特定的任务上，从而实现优异的性能。

发布之后，GPT和BERT迅速成为了NLP领域的标准算法。由于每加入一个新的任务都需要用大量的GPU进行微调，这也为许多科学家和工程师提供了大量工作机会（笑）。

2020年，GPT-3出现了。虽然这个模型在公众眼中没有特别引人注目的地方，但它完成了一件特别重要的事情，就是去掉了模型微调这个步骤。我们只需要使用GPT-3这个预训练过的模型，用简单的语言描述一下要完成的任务，比如"下面是一条大众点评网的用户评测，告诉我它的感情是正面的、中性的还是负面的"，并给出几个可选的例子，它就可以在这个任务上取得相当优异的成绩。GPT-3和最初版的GPT在结构上并没有太大不同，主要区别在于模型的规模和预训练的数据量增大了500倍。这种方法与[深度学习一直以来的暴力风格](/recent-AI-advancement.html)非常吻合，再次证明了深度学习的强大潜力。

在2022年，OpenAI发布了另一个当时并未引起广泛关注的成果——GPT Instruct。这个成果首次引入了RLHF，通过强化学习的方法实现了近乎无限规模的对齐训练过程。这个过程首先使用强化学习从有限的人类标注中学习人类对两个输出的偏好程度，然后用训练出来的模型代替人类来对模型的输出打分，参与模型的训练。这相当于获得了无穷多的训练数据，进一步增加了对齐过程的数据量。在这种新方法的支持下，OpenAI首次实现了让一个语言模型理解人类意图并根据人类指令作出回答（Instruction Following）。同年11月，基于GPT-3和GPT Instruct两个成果的结合，OpenAI推出了ChatGPT。后来的事情大家都比较熟悉了。

从这个历史来看，基础大模型到底是什么呢？我认为有两个标志性的能力：

1. 不通过模型微调就可以进行few-shot learning或者in-context learning的能力。更具体地说，一个基础大模型如果想要适配到一个新的场景中去，它不需要科学家或者工程师的参与，不需要大量的GPU来支持训练和微调，也不需要改变这个模型的权重。而只需要普通用户与其交互，就可以完成这个适配过程。这对于实际产品的形式来说是革命性的，是一种破坏性的创新（disruptive innovation）。原因是，它允许我们只需部署一个模型，就能完成许多不同的任务，这也是基础大模型中"基础"二字的由来。

    一个有趣的观察是，至少从目前来看，这些基础大模型似乎反而不适合用于模型微调。一个例子是BERT经过模型微调往往可以得到很好的结果，但CLIP这种具有Open Vocabulary跨模态理解能力的基础大模型，其模型微调的难度却大很多，在学界是臭名昭著的难以通过微调来利用。要想有效地利用CLIP，更好的办法往往是进行embedding backtracking，在不改变模型权重的情况下修改输入token。

2. [对话式UI](/GPT-knowledge-management.html)。在这种UI出现之前，如果想利用机器学习或者AI模型的能力，我们必须理解如何编写Python程序来调用一个模型，比如需要学习什么是Tokenization，如何调用PyTorch，以及如何管理CUDA和CPU之间的内存转移。但是有了对话式AI后，任何人只要会说话，就可以通过直接对话来调用其能力。这是另一个明显的产品形式的破坏性创新。

当这两者结合在一起时，就可以构成类似ChatGPT这样的革命性产品。一方面，我们不需要机器学习科学家的参与，就可以将AI应用到自己的任务上；另一方面，也不需要懂编程，就可以使用AI的各种能力。

而实现Few-shot Learning和Conversational UI这两个关键特质的技术手段则是大规模的预训练。一个非常有趣的观察是，从Transformer到GPT、到GPT-3、再到GPT-Instruct和ChatGPT，其中模型的基本单元，也就是Transformer和Self-Attention机制，从来没有改变过。变化的主要是预训练的数据规模。BERT只用了33亿个单词，但到了GPT-3，这个规模增大了500倍，而GPT-Instruct通过RLHF实现了几乎无限的对齐数据。更多的训练数据让我们能更好地覆盖非常复杂的问题解决空间，从而更好地支持各种不同的任务。

所以总的来说，我认为基础大模型的核心并不在于"大小"，"大"只是用来实现其两种核心特质的技术手段，而并非目标。从机器学习的角度来看，它的核心特征是Few-shot Learning，不需要科学家的参与，仅凭最终用户的力量就可实现任务的适配。从UI的角度来看，另一个可选的特质是对话式UI，这种界面进一步极大地降低了使用的门槛，让更多人能够受益。

<script async data-uid="65448d4615" src="https://yage.kit.com/65448d4615/index.js"></script>
