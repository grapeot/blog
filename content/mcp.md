Title: 统一工具协议的诱惑：MCP背后的商业与技术博弈
Date: 2025-03-22 22:00
Category: Computing
Tags: AI, Agentic AI, MCP, Chinese
Slug: mcp

我们在推广Agentic AI的过程中，有一个问题经常被问到：我们怎样对接MCP（Model Context Protocol）？有哪些好用的MCP Server？这篇文章就想梳理一下大家很少问，但可能更重要的问题，比如：我们为什么需要MCP？MCP提供了哪些功能，解决了哪些痛点？为什么MCP这么受欢迎？我们什么时候要用MCP？什么时候不要用？

## 工具之痛

如果你用LLM API做过Agent的开发，肯定会碰到过一个痛点：各家LLM对于工具（Agent）调用的API格式是不一样的。比如，OpenAI的API用JSON格式来描述工具的名字、作用和参数。Anthropic的API虽然格式很类似，但是在reasoning和安全性方面有一些其他的要求，所以如果直接把针对GPT写的code直接拷过来用的话，在Claude上往往会挂掉。而Gemini则是另外一种完全不同的格式。

所以为了保证Agentic AI系统对不同LLM的兼容性和灵活性，就需要针对每种不同的LLM服务商写一个专门的抽象层来适配。这对商业产品开发是一个相当大的投入，也会拖慢上线和迭代的进度。因此，业界无论是开源项目还是商业公司，都在探索一种统一的接口，把Agent用户层和LLM API层之间关于工具调用的部分给专门抽象出来，让开发人员能够一次实现，到处运行，不用改代码就可以把一个Agent在所有流行的LLM API上都跑起来。这好比给Agent的世界加入了一个USB-C接口，可以极大地推动整个领域的发展，提升大家的开发效率。

不过各家公司这么重视这个领域，并不单纯为了技术进步。它在商业上也可以带来很多现实的利益。比如，微软在成为PC操作系统的事实标准之后，在后续领域的发展中就有了极强的话语权和利润空间。类似的，苹果和谷歌在构建了App Store的生态系统之后，也从分成模式里获得了巨额利益。所以，如果一家公司可以主导这种标准的制定，就可以在里面夹带私货。一方面保证这项技术和自家的产品结合得最紧密，鼓励大家来用自家的产品；另一方面也可以从标准认证方面把它做成生态壁垒，收割红利。而对于初创团队来说，如果他们能用开源的方式拿出一个易用且轻量化的协议标准，也可以撬动那些不想被大厂绑死的用户，迅速打开局面。

这种双向博弈，让LLM工具协议领域呈现出一片诸侯争霸的局面，各家都想先跑马圈地、奠定基本盘。其中最早的方案可能是23年初发布的[ChatGPT Plugin](https://openai.com/index/chatgpt-plugins/)。它其实就是[GPTs](https://openai.com/index/introducing-gpts/)和[GPT Store](https://openai.com/index/introducing-the-gpt-store/)的技术基础。它通过manifest file和OpenAPI的方式来用一种统一的接口向LLM介绍工具和prompt。从技术上看，直到现在MCP还基本沿用了它的基本思路。不过因为种种原因，ChatGPT Plugin的协议已经被OpenAI弃用了。

后来LangChain给出了自己的[工具协议](https://python.langchain.com/docs/concepts/tools/)，凭借生态的先发和组件的丰富性吸引了不少人。类似的Pydantic也引入了一个[基于Python Decorator的协议](https://ai.pydantic.dev/tools/)，主打强类型和Pythonic的风格。在这些工具协议中，最有名的可能是大家所熟知的 [Model Context Protocol](https://modelcontextprotocol.io/)，也就是MCP。那为什么在这个竞争激烈的领域，MCP可以迅速获得先机，得到很多公司和用户的支持呢？为了理解这个问题，我们得先退一步，看一看这样的标准从技术上需要满足哪些基本要求。

![MCP framework](/images/mcp.jpeg)

## 技术优势

从纯技术角度看，这个工具协议的基本要求是提供一个抽象层，这样可以让Agentic AI的开发者一次实现到处运行。但它又不能过度抽象，比如LangChain在这方面就臭名昭著，常常被人吐槽需要跟进800个抽象类才能搞清楚要改动哪个地方。此外，它应当支持足够多的功能，因为如果开发者发现有一个功能这个协议不支持的话，他还是会去转向其他兼容性更好的协议。最后一个要求则是要方便debug，因为在典型的开发过程中，debug所花的时间会远远大于写代码本身的时间。如果你从这几个技术维度来审视的话，就会发现目前市场上没有特别优秀的产品。比如：

* OpenAI Plugin 虽然门槛低，只要会写 JSON 就行，但是高度绑定 ChatGPT，扩展性不够好。
* Langchain 虽然生态支持好，但是抽象层次不低，Agent 相关逻辑过度复杂。
* Pydantic 虽然让单个 Python 项目的开发变得很舒服，但是因为所有函数类型都在同一个库里，要想跨语言或者跨团队使用就没那么灵活。

相比之下，MCP 则在这几个维度实现了一个比较中庸的定位，没有硬伤。比如，在抽象方面，MCP 更像是一个轻量级的底层协议，告诉模型资源是什么、工具有哪些、提示词长什么样，以及怎样调用。这在抽象和过度抽象之间做了一个适当的权衡。一方面可以方便对接不同类型的服务和资源，另一方面也不会大包大揽，把多步推理、计划等高阶能力也放到协议里，维持了合理的简洁度。在表达上，MCP 支持资源、函数、Prompt 模板和 Sampling（让工具可以反向调用 LLM），让扩展空间显得还算可以。在易用性和可调试性方面，MCP 提供了多语言 SDK，强调 AI-friendly，通过提供 llm.md 让新手可以很快写出一个符合协议的服务端或者客户端。但是，由于背后仍然是一个基于 server-client 的多进程多服务架构，所以新手在调试的时候仍然需要查看协议日志、理解数据格式。对于简单需求来说，不如在同一个 Python 脚本里面写来得那么方便。官方虽然做了 Inspector 之类的调试工具，但是门槛还是很高。另外，client和server必须在一台机器上，这个限制多少也有点诡异（Anthropic提到25年可能会[把这一点改掉](https://modelcontextprotocol.io/development/roadmap)）。

## 商业因素

所以从技术的角度来说，MCP并不是第一个去解决LLM工具调用标准化这个痛点的产品，也不是技术上有压倒性优势的产品。它为什么有这么高的关注度，除了技术上没有明显短板之外，其实是三点商业上的原因：

第一，是Anthropic有备而来。它一开始亮相的时候就提供了相对完整的生态链，比如有官方文档、示例代码、多种语言SDK，以及Claude Desktop对MCP的主动支持。所以用户如果选择它，不仅能够获得一个相对简洁的统一抽象工具，而且还可以在Anthropic提供的环境里开箱即用，不用自己从0到1实现所有东西。这些特性聚拢了一批早期的开发者。

第二，Anthropic在宣传和推动方面也做足了功课。他们一方面非常强调MCP本身是开放的，不依赖于某个特定的框架或者编程语言。另一方面也联合多家厂商来集成MCP，让大家在短时间内就看到了一批落地场景，形成了示范效应。这和我们在[上一篇文章](https://yage.ai/manus.html)里介绍的复利效应很相关。对于开发者来说，这就意味着如果我写一个MCP，一下子就可以接入很多平台和工具，大大提升可见度和组合效应。这是一种非常实在的诱惑。

第三，Anthropic是领域里一个技术风评很好、也非常注重Agent的公司，所以大家会信任它在技术上有前瞻性、在资源上会持续投入。同时，Anthropic又很强调MCP是一个完全开放的协议，这就让大家对它的信心比较足，愿意投入更多的commitment。

因此，是这些技术和非技术的原因共同作用，做成了现在MCP的流行。

## MCP的使用场景

如果回到最开始我们讨论的问题，我们怎样对接MCP？很多时候大家关心MCP，其实并不是关心MCP这个协议本身，而是一定程度上把它和Agentic AI开发绑定了起来，觉得要去构建Agentic AI，就需要去学一个framework。我们[之前解释过](https://yage.ai/why-forget-all-frameworks.html)，这个思路在当前其实并不合理。像我们之前[其他文章](https://yage.ai/cursor-to-devin.html)介绍的那样，使用Agentic AI的门槛其实很低。即使不用任何框架或者协议，也可以高效地构建和迭代Agent。比如依托Cursor，我们只需要写一个Python command line，然后结合自然语言向Cursor介绍如何使用这个工具，就可以在五分钟内构建一个Agent。在我们的具体例子里是给当时的Cursor加入了搜索网络的功能。

而从前面的分析也可以看出，MCP出现的初衷是为了简化对多种LLM的适配的简单适配这个痛点。这和商业场景中更关键的问题，比如怎么找product market fit，怎么用AI解决客户的痛点等等并不是特别相关。换言之，MCP和这些关乎startup生死存亡的核心需求关系并不大。不是说用了MCP就可以发现以前忽视的客户的痛点、发现隐藏的场景，或者攻克解决不了的技术难题。相反，因为它的debug相比普通编程更加困难，这可能反而会拖慢开发速度。因此，用MCP这个决策本身可能根本就不是Agent开发过程中的核心决策。它确实重要，关乎长期的开发效率，但并不是说我要去做Agent，就得去了解MCP，这是本末倒置了。

而同时Agentic AI还是一个高速发展的领域，现在，大家对于MCP的支持也良莠不齐。比如Cursor在集成MCP server的过程中经常出现各种意外的问题。你从他们的release notes也能看出来，他们一直在修复各种MCP集成的bug。所以我们对于MCP的基本态度是：如果你只是抱着学习Agentic AI或者探索用户场景的心态，建议是先别慌把自己绑定在某种特定的技术上。而是利用Agentic AI门槛低的特点，在一两种LLM上快速迭代。等确认这是一个客户会喜欢的产品后，再利用AI把它翻译成MCP协议。这样可以方便后面继续适配多种LLM。同时，MCP在软件分发上也有自己实在的优势。很多软件，包括Claude Desktop和Cursor，只需要做一个简单的更改就可以直接使用了。所以，它可以作为一个中后期加速迭代和软件分发的渠道，而不用在产品迭代早期就引入。

## 结语

目前这一轮关于工具调用协议的竞争还处于百花齐放的状态，很难说尘埃落定。MCP只是取得了行业的领先优势，这并不意味着它已经成为了事实上的标准。回顾软件发展的历史，任何一家公司，只要能在标准层面胜出，就可以在后续的生态演进里拥有极高的话语权。因此，各大公司一定会投入大量的资源在这方面进行竞争，从而潜移默化地带领行业资源向自己的产品或平台集中。最终可能像当年浏览器和操作系统那样，呈现出几大阵营竞赛的状态。对于我们开发者来说，眼下最重要的还是先明确自己的应用需求，同时保持对MCP等工具协议的关注。毕竟，Agentic AI的技术迭代日新月异。未来有一种新的方案完全颠覆我们今天的所有协议，也是有可能的。
