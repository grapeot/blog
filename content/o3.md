---
Title: 不是我更偏爱o3，是它开始真的主动干活了
Date: 2025-04-20 20:00
Category: Computing
Tags: Chinese, Agentic AI
Slug: o3
Translation: o3-en.html
---

最近，我导出了一下自己在 ChatGPT 上的历史使用数据（顺带一提，Settings → Data Controls → Export all data 可以拿到一个 zip 压缩包，里面包含所有聊天记录的 JSON 文件，稍微写点脚本就能可视化了）。

我原本只是出于好奇，想看看过去几个月自己的用量变化，但画出的图表让我愣了一下。过去这段时间里，出现了两个明显的尖峰：一个在 2 月 26 日（图像生成上线），另一个在 3 月 25 日（o3 发布）。

![ChatGPT usage](/images/chatgpt_usage_202504.png)

而后者，也就是 o3，上线之后，我的使用量直接拉出一根近乎垂直的用量火山。这是我第一次意识到：不是我有意去尝试 o3，而是它真的改变了我的行为习惯，悄悄成为我生活工作里的默认助理。

过去我们说 ChatGPT 之类的传统 AI 是工具，是提问机器——你问，它答。但 o3 不一样。它是第一个让我意识到，我把任务交给 AI，它自己能查、能想、能试图说服我、还能准备邮件发出去的模型。它像一个真正的执行者，不只是反射器。

下面这几个我亲身经历的小故事，基本能说明这个转变是怎么发生的。它们不是最复杂的技术挑战，也不是最典型的 AI 应用场景，但正因为如此，它们才值得讲。

## 看图 → 推理 → 写邮件：修理咖啡机漏水

家里的咖啡机开始漏水。不是突然爆裂那种，而是每天用完之后，底座附近总有一些水迹。最开始我以为是垫圈老化，网上也几乎所有人都说是那个问题。

我也很快找到了零件，换上新的。可问题依然存在。更郁闷的是，这台机器已经用了好几年了，保修早过了。但我总觉得这不该是致命问题——它只是个漏水的咖啡机，不是发动机。我应该能搞定它。

于是我把这事丢给 o3。关键是：我不仅告诉它咖啡机漏水，而是像给一个人解释问题一样，把我做过的排查步骤、查过的论坛、换过的部件都详细描述了一遍。我还附了一张机器的照片和产品型号。

结果非常不一样。

o3 没有直接给我一个诊断结果，而是先做了三件事：

1. 自己去查了这台咖啡机的维修手册，包括 exploded view 的结构图。
2. 根据结构图和我之前换过的位置，它推理出另一个更可能漏水的密封件——一个上方隐藏、但如果破损会沿着管道向下渗水的零件。
3. 写出了判断理由，包括结构路径、水流方向、相邻部件编号，并提供了原始 PDF 的链接让我核实。

![Espresso machine explosive view](/images/espresso_machine_explosive_view.png)

这里面最让我惊讶的不是它判断得对不对，而是它能读懂结构图并主动推理。要知道，一般 LLM 对这类图像是无感的，但 o3 开始能把图像当作输入的一部分进行 reasoning，而不仅仅是 caption。我继续 push 它解释“为什么不是我换过的那个部件”，它回溯了爆炸图中的水路，并指出我的更换部件虽然常见，但不在推测的漏点路径上。我对照图一看，还真是。

于是我让它写一封发给售后的英文邮件，把整个排查过程、判断结果、我能否自行更换的疑问都写进去。它迅速给出草稿，并附上了品牌的客服邮箱，还提醒我确认型号和购买时间。

更绝的是，它在邮件之后还补充了一段话，建议我如果打算自己更换，需要准备一支食品级硅脂和一个特定规格的扳手，并贴上了参考购买链接。我没提过这些，它是自己判断并提醒的。这是我第一次感觉到 ChatGPT 不只是回答，而是在主动协助我完成任务。它不只是给一个靠谱的诊断建议，它是一个能和我一起收集证据 → 排除假设 → 准备沟通材料的搭档。任务不是我一个人 carry，而是我们一起做出来的。

## 动手算太麻烦的事，让 AI 去端到端推一遍

我有时候会看一些拍卖网站上的东西，比如摄影器材之类。最近看到一件在香港佳士得拍卖的商品，看起来价格不错。但问题是，拍卖价格从来不是你看到的价格：

* 要加拍卖行的佣金（往往是阶梯式的）
* 要加上币种换算（港币转美元）
* 如果是进口，还要加上海关关税、报关手续费
* 落地之后，如果发往华盛顿州，还要算上销售税

你可能觉得这都不是很难算。但对于一个对税务和进口毫无经验的人来说，这种跨界的含糊规则 + 每次都得点进去查的佣金表，真的让人下意识拖延。所以我就想：能不能让 o3 直接算一遍给我看。

我打开 ChatGPT，把商品链接贴进去，然后告诉它：“这件物品在香港拍，成交价假设是 XXX 港币，我人在华盛顿州，帮我算一下最终我会花多少钱拿到手。”

o3 做的不是单步查表，而是执行了一个 chain-of-thought 风格的完整操作链：

1. 先确认拍品的物品类别，以判断进口关税适用的 Harmonized Tariff Schedule（HTS） 条目。
2. 查找当前该类目在美国的关税税率。
3. 调用汇率数据，把港币转换为美元。
4. 根据拍卖行官网给出的阶梯式佣金率，计算手续费。
5. 模拟落地后被收取销售税的场景，估算华盛顿州的适用税率。
6. 把所有数据源链接贴出来供我交叉验证。
7. 给出最终到手价格的估算值。

关键是——我没有一步一步提问，它是自己完成了任务拆解、子模块执行和汇总表述。而且还有 bonus：它提醒我，如果关税在某日期前支付，有可能适用旧税率（因为有政策将要更新），建议我关注申报时间窗口。我自己都没意识到这点。

这已经不是让 AI 帮我算账，而是我给它一个模糊目标，它自动把流程跑了一遍，还提醒了我原本忽略的细节。以前你需要五个网页、一张表格、一个计算器和一点经验才能算清楚的 landed cost，现在只要一句话：你帮我算一下。这不是方便一点，这是整个任务完全被AI代理了。

## 我的 AI 舆情编辑室：哈佛、特朗普和背后的财政杠杆战

那天刷新闻时，我看到一条让我略有警觉的标题：“特朗普考虑切断哈佛的联邦资金，并取消其非营利地位” 一开始我并没有太在意，以为是造势的口号。但紧接着，新闻里又提到限制国际学生名额、可能撤销 503(c) 非营利地位，还伴随着哈佛开始变卖部分校产的报道。我突然意识到，这不是口水仗，这可能是对美国一整套顶级高校财政模型的打击。

于是我把这个问题交给了 o3，问它能不能帮我：

* 回顾这件事的时间线
* 解释涉及到的法律条款，比如 503(c)、国际学生资助机制、联邦科研拨款标准
* 对比历史上有没有类似的操作或政策变动
* 分析这对哈佛，以及金融市场的可能影响

我没抱太大希望——因为这种跨法律、政策、社会舆论的复杂新闻，往往是 AI 的短板。但 o3 的处理让我大受震撼。

它先列出了最近一个月的相关新闻源，做成一个小型时间线，解释了这个风波的来龙去脉；接着用简洁明晰的语言解释了 503(c) 非营利地位的定义、适用条件，以及一旦取消会导致哪些税收和财政结构上的变化。它还指出：哈佛虽然富有，但其运营高度依赖联邦科研经费，如果这部分资金断掉，不仅会影响项目运行，连国际博士生奖学金也会受到连锁影响。他还用22年英国养老金做类比，分析了哈佛出卖校产对金融市场的可能影响。

我原本只是想了解一下情况，它却像一个结构化的舆情分析师，把问题从八卦拖进了财政武器层面。而且整份报告结构分明、没有情绪色彩，也没有站边，只是条理清晰地推演了“如果真动手，会发生什么”。你可以把它想象成一个立场中立但能力极强的实习政策研究员，不会喧宾夺主，但会把你引到更高维度的问题空间里。对我们这种对美国政治经济体制不熟悉的人尤其有帮助。

## 皮具保养的意外惊喜

事情的起因很简单：我车里的皮革内饰有点脏，想买一个皮具清洁套装来打理一下。我原本打算买品牌官网推荐的那一套，图个省事，看到价格吓了一跳。他明明可以用抢的，还送你一套皮具清洁剂，真是感动。

我把车型贴给 o3，然后问它有没有推荐的品牌和产品。它照例先列出几种产品，包括官网推荐款。但它没有止步于此。它继续爬了一些论坛、产品说明书，接着指出：“你看的这款其实是由某某公司贴牌生产的，你可以直接买它原始品牌版本，配方几乎一样，价格便宜20倍。”

我当时都愣住了。因为我完全没想到贴牌这回事，自然也没在prompt里面提到。它是自己分析出了这款产品的 OEM 来源，还附了配方对比表和化学成分说明书，最后贴出替代品链接让我自己确认。

这件事让我第一次意识到：o3 这种 Agentic AI 不只是机械完成目标，它甚至开始主动优化目标函数。它理解我想要的其实是好的清洁效果 + 不被品牌溢价收割，所以它基于任务意图，绕开了我的 prompt 的字面含义，而给了我一个更聪明的选择。我没有教它消费心理学，它却开始做我认同的消费判断；我没有说帮我省钱，它自己理解了我不想多花冤枉钱的含义。

这不是一个信息工具，这是一个正在习得我个（diao）人（si）品味的 agent。

## Agentic AI 的临界点，不在于能不能，而在于会不会自己动

从这些案例我越来越清楚地意识到：过去我们把 AI 当成回答机，从 prompt 出发，希望它能准确响应。而 o3 带来的变化，是它开始自己提出行动步骤、补全任务结构、提醒你遗漏的部分、甚至改写你的目标本身。这不仅仅是一个语言模型能力的提升，更是任务结构权限的变化。

AI 从“你问我答”变成“我来帮你一起完成”，这个转变的节点非常值得我们记录下来。它让很多原本你不会启动的事变得可启动；它让很多你不想重复思考的流程变得可复用；它让一些你原以为只有人能完成的工作，开始变成“AI＋我”合作完成。

而这所有的变化，最后落在一个很现实的指标上：我对它的使用频率暴涨了。

我在写这篇文章的时候，不断反思这些案例的共同点。不是 AI 更强，而是：

* 它开始主动连接信息 → 行动 → 反馈 → 外部表达
* 它不再等你发 prompt，而是跟得上你思考的节奏
* 它不需要你指令明确，而是自己补全任务模型
* 它不仅服务复杂问题，也改变了日常决策的起点

你有没有过类似的经历？本来没打算让 AI 做，但后来发现它不但能做，而且比你想象中聪明？也欢迎把你的故事、使用路径、截图、推理链分享在下面的评论里。